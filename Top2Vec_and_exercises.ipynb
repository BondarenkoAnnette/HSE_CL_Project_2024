{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d58b434-aa7f-4870-a4b3-d310e766aca3",
   "metadata": {},
   "source": [
    "Topic modeling with Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", '3.0.0', ignore_verifications=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c211dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_dataset_split_names\n",
    "get_dataset_split_names(\"cnn_dailymail\", '3.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff0d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c72284",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cnn_dailymail\", '3.0.0', split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc37079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(dataset)\n",
    "df = dataframe.loc[0:9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56de310a-8ae8-4cf2-af73-2cdad303e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\kayir\\Documents\\project_data\\my_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f81f6da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "articles = df[\"article\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010cdc5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Top2Vec(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade4cab0-029e-434c-bcaf-e3d74c3ada51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mymodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98eafbb-1e8d-4b89-ad6e-abc0cc0c98aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print(topic_sizes)\n",
    "#список всех тем - тема с индексом 0 самая популярная и по ней есть 313 документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab4ea93-700a-462a-9ebd-ecf334f4adc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(topic_nums)\n",
    "#в модели 115 разныx тем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e4721-4949-4f06-b97a-a12663d40651",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048ca28c-edce-4adf-9083-9019e59266df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for words, scores, num in zip(topic_words, word_scores, topic_nums):\n",
    "    print(num)\n",
    "    print(f\"Words: {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c6d1f-ae41-4731-b22f-2ba0089069b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ПОИСК ТЕМ ПО КЛЮЧЕВОМУ СЛОВУ\n",
    "topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=[\"music\"], num_topics=5)\n",
    "print(topic_nums)\n",
    "print(topic_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78046155-90c5-40da-8ad3-75ec04b68fd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ПОСМОТРЕТЬ ДОКУМЕНТЫ ПО ТЕМЕ, ЗНАЯ ЕЕ НОМЕР\n",
    "documents, document_scores, document_ids = model.search_documents_by_topic(topic_num = 19, num_docs = 5)\n",
    "for doc, score, doc_id in zip(documents, document_scores, document_ids):\n",
    "    print(f\"Document:{doc_id}, Score: {score}\")\n",
    "    print(\"----------\")\n",
    "    print(doc)\n",
    "    print(\"----------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e8511-6958-4225-a76d-9d8aee65b0b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ЗАПИСАТЬ РЕЗУЛЬТАТЫ В ДАТАФРЕЙМ\n",
    "topic_of_interest = 5\n",
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=topic_of_interest, num_docs=topic_sizes[topic_of_interest])\n",
    "\n",
    "results = list(zip(documents, document_scores, document_ids))\n",
    "results_df = pd.DataFrame(results, columns = ['documents', 'document_scores', 'document_ids'])\n",
    "results_df = results_df.drop_duplicates(subset=\"documents\", keep='first')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fdf049-594b-45e8-a4fd-b713dd283f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(r'C:\\Users\\kayir\\Documents\\project_data\\politics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b90b26-7846-4e72-b196-ddf5bf693b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ОБЛАКА СЛОВ\n",
    "for topic in topic_nums:\n",
    "    model.generate_topic_wordcloud(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca2902f-23db-407f-bbc3-1aee34c32a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ПОИСК ДОКУМЕНТОВ ПО КЛЮЧЕВЫМ СЛОВАМ\n",
    "documents, document_scores, document_ids = model.search_documents_by_keywords(keywords=[\"healthcare\",\"cure\"], num_docs=5)\n",
    "for doc, score, doc_id in zip(documents, document_scores, document_ids):\n",
    "    print(f\"Document: {doc_id}, Score: {score}\")\n",
    "    print(\"-----------\")\n",
    "    print(doc)\n",
    "    print(\"-----------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8805b0c9-267e-4174-8c3a-d40a78fb3ef0",
   "metadata": {},
   "source": [
    "Далее идет создание упражнений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94b8b81-1562-4a9e-ad2a-bba87ea0a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция для создания упражнения \n",
    "def exercise(text):\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', text) #убираем пунктуацию\n",
    "    text_tokenized = word_tokenize(clean_text) #токенизируем\n",
    "    text_tagged = nltk.pos_tag(text_tokenized) #тэгируем, но теперь надо создать список слов с тэгами\n",
    "    list_of_tagged = [] #создаем список всех слов с \"приклеенными\" тэгами,который можно перебирать\n",
    "    for elem in text_tagged:\n",
    "        word_tagged = '_'.join(elem)\n",
    "        list_of_tagged.append(word_tagged)\n",
    "    tagged_string = ' '.join(list_of_tagged) #объединяем в строку, чтобы можно было пройтись регуляркой\n",
    "    \n",
    "    verbs_past_participle = re.findall('[a-zA-Z]+_VBN', tagged_string)\n",
    "    verbs_gerund = re.findall('[a-zA-Z]+_VBG', tagged_string)\n",
    "    verbs_past_tense = re.findall('[a-zA-Z]+_VBD', tagged_string)\n",
    "    verbs_present_3rdperson = re.findall('[a-zA-Z]+_VBZ', tagged_string)\n",
    "    all_verbs = verbs_past_participle + verbs_gerund + verbs_past_tense + verbs_present_3rdperson #список глаголов из текста (кроме базовых форм)\n",
    "    verbs_string = ' '.join(all_verbs)\n",
    "    verbs_clean = re.sub(r'_[a-zA-Z]*', '', verbs_string)\n",
    "    \n",
    "    verb_lemmas = [] #создаем список лемматизированных глаголов \n",
    "    doc = nlp(verbs_clean)\n",
    "    for token in doc:\n",
    "        verb_lemma = token.lemma_\n",
    "        verb_lemmas.append(verb_lemma)\n",
    "        \n",
    "    new_text = text\n",
    "    for verb in verb_lemmas: #собственно цикл, убирающий из оригинального текста предлоги и заменяющий их на пробелы\n",
    "       reg_exp = fr\"\\s{verb}\\s\"\n",
    "       new_text = re.sub(reg_exp, \" ___ \", new_text)\n",
    "    \n",
    "    str = \"Fill in the gaps with the verbs in the correct form:\"\n",
    "\n",
    "    return(str, verb_lemmas, new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4be0ba-6b6a-42f8-95f3-85dc5d007a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#нужно загрузить наши датафреймы и взять из них только элементы из колонки documents \n",
    "education_df = pd.read_csv(r\"https://raw.githubusercontent.com/BondarenkoAnnette/HSE_CL_Project_2024/main/data/education.csv\")\n",
    "education = education_df[\"documents\"].tolist()\n",
    "\n",
    "fashion_df = pd.read_csv(r\"https://raw.githubusercontent.com/BondarenkoAnnette/HSE_CL_Project_2024/main/data/fashion.csv\")\n",
    "fashion = fashion_df[\"documents\"].tolist()\n",
    "\n",
    "food_df = pd.read_csv(r\"https://raw.githubusercontent.com/BondarenkoAnnette/HSE_CL_Project_2024/main/data/food.csv\")\n",
    "food = food_df[\"documents\"].tolist()\n",
    "\n",
    "jobs_df = pd.read_csv(r\"https://raw.githubusercontent.com/BondarenkoAnnette/HSE_CL_Project_2024/main/data/jobs.csv\")\n",
    "jobs = jobs_df[\"documents\"].tolist()\n",
    "\n",
    "health_df = pd.read_csv(r\"https://raw.githubusercontent.com/BondarenkoAnnette/HSE_CL_Project_2024/main/data/health.csv\")\n",
    "health = health_df[\"documents\"].tolist()\n",
    "\n",
    "sport_df = pd.read_csv(r\"https://raw.githubusercontent.com/BondarenkoAnnette/HSE_CL_Project_2024/main/data/sport.csv\")\n",
    "sport = sport_df[\"documents\"].tolist()\n",
    "\n",
    "films_df = pd.read_csv(r\"https://raw.githubusercontent.com/BondarenkoAnnette/HSE_CL_Project_2024/main/data/films.csv\")\n",
    "films = films_df[\"documents\"].tolist()\n",
    "\n",
    "environment_df = pd.read_csv(r\"https://raw.githubusercontent.com/BondarenkoAnnette/HSE_CL_Project_2024/main/data/environment.csv\")\n",
    "environment = environment_df[\"documents\"].tolist()\n",
    "\n",
    "technology_df = pd.read_csv(r\"https://raw.githubusercontent.com/BondarenkoAnnette/HSE_CL_Project_2024/main/data/technology.csv\")\n",
    "technology = technology_df[\"documents\"].tolist()\n",
    "\n",
    "tourism_df = pd.read_csv(r\"https://raw.githubusercontent.com/BondarenkoAnnette/HSE_CL_Project_2024/main/data/tourism.csv\")\n",
    "tourism = tourism_df[\"documents\"].tolist()\n",
    "\n",
    "culture_df = pd.read_csv(r\"https://raw.githubusercontent.com/BondarenkoAnnette/HSE_CL_Project_2024/main/data/culture.csv\")\n",
    "culture = culture_df[\"documents\"].tolist()\n",
    "\n",
    "music_df = pd.read_csv(r\"https://raw.githubusercontent.com/BondarenkoAnnette/HSE_CL_Project_2024/main/data/music.csv\")\n",
    "music = music_df[\"documents\"].tolist()\n",
    "\n",
    "politics_df = pd.read_csv(r\"https://raw.githubusercontent.com/BondarenkoAnnette/HSE_CL_Project_2024/main/data/politics.csv\")\n",
    "politics = politics_df[\"documents\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d1a556-4407-4048-b5bd-ff0f08d53e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [politics, music, films, sport, culture, technology, environment, health, jobs, food, fashion, education]\n",
    "\n",
    "strings = [\"politics\", \"music\", \"films\", \"sport\", \"culture\", \"technology\", \"environment\", \"health\", \"jobs\", \"food\", \"fashion\", \"education\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934888b9-57e8-4fdd-8663-0eda5963f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Choose one of the following topics for your exercise:\")\n",
    "print(\"politics, music, films, sport, culture, technology, environment, health, jobs, food, fashion, education\")\n",
    "\n",
    "selected_topic = input() \n",
    "\n",
    "def checking(selected_topic):\n",
    "    for i in range(0,12):\n",
    "        if selected_topic == strings[i]:\n",
    "            topic_list = topics[i]\n",
    "            random_article = random.choice(topic_list)\n",
    "            ex = exercise(random_article)\n",
    "            answer = input(\"Do you want to see the article?\")\n",
    "            if answer == \"yes\":\n",
    "                return ex, random_article\n",
    "            else:\n",
    "                return ex\n",
    "\n",
    "print(checking(selected_topic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
